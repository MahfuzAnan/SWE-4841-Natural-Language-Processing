{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1 — Clean install & GPU check (Colab-safe)"
      ],
      "metadata": {
        "id": "hndg8MESfRcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi -L || print(\"⚠️ No GPU detected. Go to Runtime > Change runtime type > GPU.\")\n",
        "!nvidia-smi -L\n",
        "# Remove RAPIDS if present (they pin old pyarrow)\n",
        "!pip -q uninstall -y cudf-cu12 dask-cudf-cu12 cuml-cu12 pylibcudf-cu12 rmm-cu12 ucx-py ucxx rapids-dask-dependency || true\n",
        "# Colab-friendly versions\n",
        "!pip -q install -U pandas==2.2.2 pyarrow==21.0.0\n",
        "!pip -q install -U transformers datasets accelerate scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moNRDhJGfU4Q",
        "outputId": "bef75a80-495e-432d-9707-5a7d143f7b13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-0277f83b-4ecc-2b8e-1778-611e3432ed3e)\n",
            "\u001b[33mWARNING: Skipping ucx-py as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ucxx as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2 — (Optional) Mount Drive"
      ],
      "metadata": {
        "id": "o69MJF7efeR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "USE_DRIVE = False  # set True if you want to save to Drive\n",
        "if USE_DRIVE:\n",
        "    drive.mount('/content/drive')\n",
        "    SAVE_DIR = \"/content/drive/MyDrive/baitbuster_two_stage\"\n",
        "else:\n",
        "    SAVE_DIR = \"/content/baitbuster_two_stage\"\n",
        "\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(\"Saving to:\", SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6OnFd6tffTD",
        "outputId": "6049c998-21bc-4b89-c998-48555ba89899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to: /content/baitbuster_two_stage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3 — Load dataset"
      ],
      "metadata": {
        "id": "41NV-r_ifxpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, os\n",
        "\n",
        "CSV_PATH     = \"/content/drive/MyDrive/Dataset/BaitBuster-Bangla_253070_18c_HL10k_AIL.csv\"\n",
        "PARQUET_PATH = \"/content/drive/MyDrive/Dataset/BaitBuster-Bangla_253070_18c_HL10k_AIL.parquet\"\n",
        "XLSX_PATH    = \"/content/drive/MyDrive/Dataset/BaitBuster-Bangla_253070_18c_HL10k_AIL.xlsx\"\n",
        "\n",
        "if   os.path.exists(CSV_PATH):     df_all = pd.read_csv(CSV_PATH)\n",
        "elif os.path.exists(PARQUET_PATH): df_all = pd.read_parquet(PARQUET_PATH)\n",
        "elif os.path.exists(XLSX_PATH):    df_all = pd.read_excel(XLSX_PATH)\n",
        "else: raise FileNotFoundError(\"Upload dataset and update the paths above.\")\n",
        "\n",
        "print(\"Columns:\", list(df_all.columns)[:30])\n",
        "print(\"Rows:\", len(df_all))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBoLJNIkfzmo",
        "outputId": "b409829e-3b73-409c-b5fa-abbeddfef3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns: ['channel_id', 'channel_name', 'channel_url', 'video_id', 'publishedAt', 'title', 'title_debiased', 'description', 'description_debiased', 'url', 'viewCount', 'commentCount', 'likeCount', 'dislikeCount', 'thumbnail', 'auto_labeled', 'human_labeled', 'ai_labeled']\n",
            "Rows: 253070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4 — Labels, text cleaning, engagement features\n"
      ],
      "metadata": {
        "id": "6XXHNNl4f__w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prefer debiased columns when available\n",
        "TITLE_COL = \"title_debiased\" if \"title_debiased\" in df_all.columns else \"title\"\n",
        "DESC_COL  = \"description_debiased\" if \"description_debiased\" in df_all.columns else \"description\"\n",
        "\n",
        "assert TITLE_COL in df_all.columns, f\"Missing {TITLE_COL}\"\n",
        "if DESC_COL not in df_all.columns:\n",
        "    df_all[DESC_COL] = \"\"\n",
        "\n",
        "def norm_label(s):\n",
        "    if pd.isna(s): return None\n",
        "    s = str(s).strip().lower()\n",
        "    mapping = {\n",
        "        \"not_clickbait\":\"not clickbait\", \"non-clickbait\":\"not clickbait\",\n",
        "        \"non_clickbait\":\"not clickbait\", \"notclickbait\":\"not clickbait\",\n",
        "        \"click bait\":\"clickbait\", \"yes\":\"clickbait\", \"no\":\"not clickbait\",\n",
        "        \"1\":\"clickbait\", \"0\":\"not clickbait\"\n",
        "    }\n",
        "    return mapping.get(s, s if s in [\"clickbait\",\"not clickbait\"] else None)\n",
        "\n",
        "def choose_label_and_source(row):\n",
        "    for col, src in [(\"human_labeled\",\"human\"), (\"ai_labeled\",\"ai\"), (\"auto_labeled\",\"auto\")]:\n",
        "        if col in row.index and pd.notna(row[col]):\n",
        "            lab = norm_label(row[col])\n",
        "            if lab in [\"clickbait\",\"not clickbait\"]:\n",
        "                return lab, src\n",
        "    return None, None\n",
        "\n",
        "labs, srcs = [], []\n",
        "for _, r in df_all.iterrows():\n",
        "    l, s = choose_label_and_source(r)\n",
        "    labs.append(l); srcs.append(s)\n",
        "\n",
        "df_all[\"label_str\"] = labs\n",
        "df_all[\"label_source\"] = srcs\n",
        "df_all = df_all[df_all[\"label_str\"].isin([\"clickbait\",\"not clickbait\"])].copy()\n",
        "df_all[\"label\"] = (df_all[\"label_str\"] == \"clickbait\").astype(int)\n",
        "\n",
        "def _clean_text(s):\n",
        "    if pd.isna(s): return \"\"\n",
        "    return str(s).strip()\n",
        "\n",
        "df_all[TITLE_COL] = df_all[TITLE_COL].apply(_clean_text)\n",
        "df_all[DESC_COL]  = df_all[DESC_COL].apply(_clean_text)\n",
        "df_all = df_all[df_all[TITLE_COL].ne(\"\")].copy()\n",
        "\n",
        "print(\"Usable rows:\", len(df_all))\n",
        "print(\"Label balance 0/1:\", np.bincount(df_all[\"label\"]))\n",
        "print(df_all[\"label_source\"].value_counts(dropna=False).to_frame(\"count\"))\n",
        "\n",
        "# ---------- Engagement feature engineering ----------\n",
        "for c in [\"viewCount\", \"likeCount\", \"commentCount\", \"dislikeCount\"]:\n",
        "    if c not in df_all.columns:\n",
        "        df_all[c] = 0\n",
        "\n",
        "for c in [\"viewCount\", \"likeCount\", \"commentCount\", \"dislikeCount\"]:\n",
        "    df_all[c] = pd.to_numeric(df_all[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "for c in [\"viewCount\", \"likeCount\", \"commentCount\", \"dislikeCount\"]:\n",
        "    df_all[f\"log1p_{c}\"] = np.log1p(df_all[c])\n",
        "\n",
        "df_all[\"like_ratio\"]   = df_all[\"likeCount\"] / (df_all[\"likeCount\"] + df_all[\"dislikeCount\"] + 1.0)\n",
        "df_all[\"comment_rate\"] = df_all[\"commentCount\"] / (df_all[\"viewCount\"] + 1.0)\n",
        "\n",
        "NUM_FEATS = [\n",
        "    \"log1p_viewCount\", \"log1p_likeCount\", \"log1p_commentCount\", \"log1p_dislikeCount\",\n",
        "    \"like_ratio\", \"comment_rate\"\n",
        "]\n",
        "print(\"Numeric features:\", NUM_FEATS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2076
        },
        "id": "Bh5nCkxKgDL6",
        "outputId": "8ca26b82-b70e-497f-f116-cb50e28417c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total usable rows (any label): 253070\n",
            "Label balance 0/1: [208024  45046]\n",
            "               count\n",
            "label_source        \n",
            "ai            243070\n",
            "human          10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      title_debiased  label_str label_source\n",
              "0  এইমাত্র! মসজিদে নামাজরত অবস্থায় তিন বৃদ্ধকে পি...  clickbait        human\n",
              "1  ১০ বছরের সন্তান ফেলে আ,লীগ নেতার সাথে পালিয়ে গ...  clickbait        human\n",
              "2  এই মাত্র পাওয়া খবর! ৫ বছরের জেল হচ্ছে পরীমনির!...  clickbait        human\n",
              "3  ছি ছি! ভাগিনার সাথে পরকীয়ার সময় হাতেনাতে ধরা খ...  clickbait        human\n",
              "4  হায়রে পরীমনি! কারাগারে গিয়েও ভালো হলোনা! কারাগ...  clickbait        human"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2166bed4-6cf4-4709-ac58-69294ec6a029\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title_debiased</th>\n",
              "      <th>label_str</th>\n",
              "      <th>label_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>এইমাত্র! মসজিদে নামাজরত অবস্থায় তিন বৃদ্ধকে পি...</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>১০ বছরের সন্তান ফেলে আ,লীগ নেতার সাথে পালিয়ে গ...</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>এই মাত্র পাওয়া খবর! ৫ বছরের জেল হচ্ছে পরীমনির!...</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ছি ছি! ভাগিনার সাথে পরকীয়ার সময় হাতেনাতে ধরা খ...</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>হায়রে পরীমনি! কারাগারে গিয়েও ভালো হলোনা! কারাগ...</td>\n",
              "      <td>clickbait</td>\n",
              "      <td>human</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2166bed4-6cf4-4709-ac58-69294ec6a029')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2166bed4-6cf4-4709-ac58-69294ec6a029 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2166bed4-6cf4-4709-ac58-69294ec6a029');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-61f39ad7-43e0-4488-b050-093ce141a91b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61f39ad7-43e0-4488-b050-093ce141a91b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-61f39ad7-43e0-4488-b050-093ce141a91b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_all[[TITLE_COL,\\\"label_str\\\",\\\"label_source\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title_debiased\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u09e7\\u09e6 \\u09ac\\u099b\\u09b0\\u09c7\\u09b0 \\u09b8\\u09a8\\u09cd\\u09a4\\u09be\\u09a8 \\u09ab\\u09c7\\u09b2\\u09c7 \\u0986,\\u09b2\\u09c0\\u0997 \\u09a8\\u09c7\\u09a4\\u09be\\u09b0 \\u09b8\\u09be\\u09a5\\u09c7 \\u09aa\\u09be\\u09b2\\u09bf\\u09df\\u09c7 \\u0997\\u09c7\\u09b2 \\u098f\\u09ae\\u09aa\\u09bf\\u09b0 \\u09ae\\u09c7\\u09df\\u09c7! \\u09b8\\u09ac \\u09b9\\u09be\\u09b0\\u09bf\\u09df\\u09c7 \\u0995\\u09be\\u0981\\u09a6\\u099b\\u09c7 \\u098f\\u09ae\\u09aa\\u09bf \\u0993 \\u099c\\u09be\\u09ae\\u09be\\u0987\",\n          \"\\u09b9\\u09be\\u09df\\u09b0\\u09c7 \\u09aa\\u09b0\\u09c0\\u09ae\\u09a8\\u09bf! \\u0995\\u09be\\u09b0\\u09be\\u0997\\u09be\\u09b0\\u09c7 \\u0997\\u09bf\\u09df\\u09c7\\u0993 \\u09ad\\u09be\\u09b2\\u09cb \\u09b9\\u09b2\\u09cb\\u09a8\\u09be! \\u0995\\u09be\\u09b0\\u09be\\u0997\\u09be\\u09b0\\u09c7\\u0993 \\u09ae\\u09a6 \\u0996\\u09be\\u099a\\u09cd\\u099b\\u09c7 \\u09aa\\u09b0\\u09c0\\u09ae\\u09a8\\u09bf! \\u09a6\\u09c7\\u0996\\u09c1\\u09a8 \\u09b8\\u09a4\\u09cd\\u09af\\u09bf \\u09a8\\u09be\\u0995\\u09bf \\u0997\\u099c\\u09ac? Pori Moni\",\n          \"\\u098f\\u0987 \\u09ae\\u09be\\u09a4\\u09cd\\u09b0 \\u09aa\\u09be\\u0993\\u09df\\u09be \\u0996\\u09ac\\u09b0! \\u09eb \\u09ac\\u099b\\u09b0\\u09c7\\u09b0 \\u099c\\u09c7\\u09b2 \\u09b9\\u099a\\u09cd\\u099b\\u09c7 \\u09aa\\u09b0\\u09c0\\u09ae\\u09a8\\u09bf\\u09b0! \\u09ac\\u09be\\u0981\\u099a\\u09be\\u09a4\\u09c7 \\u09aa\\u09be\\u09b0\\u09b2\\u09cb\\u09a8\\u09be \\u09a8\\u09be\\u09df\\u0995 \\u09b6\\u09be\\u0995\\u09bf\\u09ac \\u0996\\u09be\\u09a8\\u0993\\u0964 Pori Moni News\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_str\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"clickbait\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"human\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x7cd0a6c84260>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Categorical distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2447 (\\N{BENGALI LETTER E}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Matplotlib currently does not support Bengali natively.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2439 (\\N{BENGALI LETTER I}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2478 (\\N{BENGALI LETTER MA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2494 (\\N{BENGALI VOWEL SIGN AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2468 (\\N{BENGALI LETTER TA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2509 (\\N{BENGALI SIGN VIRAMA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2480 (\\N{BENGALI LETTER RA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2474 (\\N{BENGALI LETTER PA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2451 (\\N{BENGALI LETTER O}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2527 (\\N{BENGALI LETTER YYA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2454 (\\N{BENGALI LETTER KHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2476 (\\N{BENGALI LETTER BA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2539 (\\N{BENGALI DIGIT FIVE}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2459 (\\N{BENGALI LETTER CHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2503 (\\N{BENGALI VOWEL SIGN E}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2460 (\\N{BENGALI LETTER JA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2482 (\\N{BENGALI LETTER LA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2489 (\\N{BENGALI LETTER HA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2458 (\\N{BENGALI LETTER CA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2496 (\\N{BENGALI VOWEL SIGN II}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2472 (\\N{BENGALI LETTER NA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2495 (\\N{BENGALI VOWEL SIGN I}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2433 (\\N{BENGALI SIGN CANDRABINDU}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2507 (\\N{BENGALI VOWEL SIGN O}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2453 (\\N{BENGALI LETTER KA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2486 (\\N{BENGALI LETTER SHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2404 (\\N{DEVANAGARI DANDA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Matplotlib currently does not support Devanagari natively.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2488 (\\N{BENGALI LETTER SA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2470 (\\N{BENGALI LETTER DA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2437 (\\N{BENGALI LETTER A}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2469 (\\N{BENGALI LETTER THA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2499 (\\N{BENGALI VOWEL SIGN VOCALIC R}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2471 (\\N{BENGALI LETTER DHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2463 (\\N{BENGALI LETTER TTA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2479 (\\N{BENGALI LETTER YA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2497 (\\N{BENGALI VOWEL SIGN U}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2455 (\\N{BENGALI LETTER GA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2456 (\\N{BENGALI LETTER GHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2524 (\\N{BENGALI LETTER RRA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2477 (\\N{BENGALI LETTER BHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2535 (\\N{BENGALI DIGIT ONE}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2534 (\\N{BENGALI DIGIT ZERO}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2475 (\\N{BENGALI LETTER PHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n",
            "/usr/local/lib/python3.12/dist-packages/google/colab/_quickchart_lib.py:32: UserWarning: Glyph 2438 (\\N{BENGALI LETTER AA}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "_df_0.groupby('title_debiased').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-b7ddb3a0-b9f1-4586-af99-0c69894ea06e\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABHUAAAGZCAYAAAAHLA1zAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAP\n",
              "YQAAD2EBqD+naQAAMWZJREFUeJzt3X90VOWdP/DPJGgoID/Wb1XEShZR6w8wBeqKi4oB2+iitYJV\n",
              "qytBKli1VhRLadeCXU9Zu2haSv3Bihtti7oLLrXUUksRVxQsYqkKlbZCtCu0gkUIsIQfme8fHmcz\n",
              "CYkJTTK54fU6J+eQ5z4zz+c+984kvHPvM6l0Op0OAAAAABIlL9cFAAAAANB0Qh0AAACABBLqAAAA\n",
              "ACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQCAdu673/1u\n",
              "rksAAFqAUAcAoJ178803c10CANAChDoAAAAACSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQ\n",
              "QEIdAAAAgAQS6gAAAAAkkFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAA\n",
              "AJBAQh0AAACABBLqAAAAACSQUAcAAAAggVLpdDqd6yIAAGg5/S/oGoP/sUeuywCAduWBK97MdQmu\n",
              "1AEAAABIIqEOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCBhDoAAAAA\n",
              "CSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQQEIdAAAAgAQS6gAAAAAkkFAHAAAAIIE65LqA\n",
              "9mjWrFkxd+7cyMv7v8xs586d8fjjj8eNN94YO3bsyOrfp0+fuPfee7Parr/++li3bl1WW+fOnWPm\n",
              "zJlx2WWXRadOnTLt1dXVMWrUqIiIJo87duzYmDhxYhQUFGTa9+zZExMmTIgRI0Zk2jZu3Nhq465c\n",
              "uTKWLVuW9Zjt27fH0qVLY8iQIdGlS5esbYMHD44pU6ZktY0cObJNjdvSx7dnz56ZtgULFkRZWVkc\n",
              "csghmbaqqqqYPn16zJ49u1XOq1ydz23xddTQeVXTgbxvJO3125L7+2HjFhYWxvz586OoqCirz4HO\n",
              "TVt7zSXtXPBe3jLHFwA4+Ah1WsCGDRti1qxZUVhYmGmbOnVqVFVVxWGHHRbz5s3L6l9aWlrnOXbu\n",
              "3BkLFy6s06+qqiqKi4tj6tSpmfaKioooLy+PiGjyuJWVlTF69OisGpYsWRIVFRVZfVtz3PXr1+93\n",
              "3yMi+vbtmxmz9raa2tq4tTX38a1p8+bNcfvtt8fQoUMzbeXl5VFZWdlq51VL729bG/dAz6uaDuR9\n",
              "I2mv35bc38aOW1t7ec0l7VzwXt4yxxcAOPgIdQAA2pGqqqo64WN1dTpH1QAALcmaOgAA7ci0adOi\n",
              "W7duWV+b3tid67IAgBYg1AEAaEcmT54cW7duzfr66HGH5rosAKAFuP0KgHbP2iMcTAoKCrIWVY6I\n",
              "yMtL5agaAKAluVIHWsnMmTNj8uTJuS4DAACAdkKoA61kzZo1cfjhh+e6DDgoXXDBBbF27dpclwEA\n",
              "AM3K7VfQSl555ZW46667cl0GHJSeeuqpXJcAAADNTqjTAvLy8qK0tDQ6duyYaVu3bl2MGTMmVq9e\n",
              "HSUlJVn9O3Soexg2b95cp9+7774bqVQq5syZE8uXL8+079q1K4qLiyMimjxuKpWKsrKyeOyxxzLt\n",
              "W7ZsiRtuuCGrb2uOm5+fX+cxa9asiYiIxYsX19nWq1evqK2tjRsRsXTp0kxbcx/fmvLy8mLSpEnR\n",
              "o0ePTNvGjRtjxowZrXZe1dbexz3Q86qmA3nfSNrrtyX3t7Hj1tZeXnNJOxeS/F5eU1s7vgDAwSeV\n",
              "TqfTuS4CAICW0/+CrjH4H3t8eEcAoNEeuOLNXJdgTR0AAACAJBLqAAAAACSQUAcAAAAggYQ6AAAA\n",
              "AAkk1AEAAABIIKEOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJFCHXBcAAEDLGv7x\n",
              "L8Q9V9yT6zIAgGbmSh0AAACABBLqAAAAACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAIJ\n",
              "dQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCBhDoAAAAACSTUAQAAAEggoQ4AAABA\n",
              "Agl1AAAAABJIqAMAAACQQEIdAAAAgAQS6gAAAAAkkFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAA\n",
              "AEACCXUAAAAAEkioAwAAAJBAQh0AAACABBLqAAAAACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEO\n",
              "AAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCBhDoAAAAACdQh1wUAANCy\n",
              "djz8Uvxp4b/mugwAaFeOWnNbrktwpQ4AAABAEgl1AAAAABJIqAMAAACQQEIdAAAAgAQS6gAAAAAk\n",
              "kFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAAAJBAQh0AAACABBLqAAAA\n",
              "ACSQUAcAAAAggYQ6AAAAAAnUoSWffNasWTF37tzIy/u/7Gjnzp3x+OOPx4033hg7duzI6t+nT58Y\n",
              "O3ZsTJw4MQoKCjLte/bsiQkTJsTKlStj2bJlWY/Zvn17LF26NIYMGRJdunTJ2jZ48OCYMmVKVtvI\n",
              "kSNbZdyBAwdGWVlZHHLIIZn2qqqqmD59esyePTvWrVuX9ZjOnTvHvHnzIiIilUrFli1bonv37ll9\n",
              "Vq5cmfMaZ86cGZdddll06tQp015dXR2jRo2KiGjy8b733nuz2q6//vpWGbeh4z1ixIhM28aNG9v9\n",
              "uA2dOxERhYWFMX/+/CgqKsrq01ZqbK3ze9y4cVn9D2TcgQMHZtra+uu5Ofa3Nd9HKioqoqioKN57\n",
              "772obcGCBW2ixpq817XMuPfee2+MHz8+li5dGrt3747zzjsvvv/970cqlcrqu2TJkjj//PPjxBNP\n",
              "jOrq6ujatWvce++90b9//2iKb3zjG3HiiSfGlVdeWWdbYWFh7Ny5M95+++3MuffMM89EcXFxfPnL\n",
              "X47vfOc7TRqrpvvvvz8qKyvjtttuO+DnAADahxYNdTZs2BCzZs2KwsLCTNvUqVOjqqoqDjvssEyI\n",
              "8YHS0tKorKyM0aNHR2lpaaZ9yZIlUVFREevXr4+FCxfWeUxERN++faO8vHy/22pqrXF79+4dt99+\n",
              "ewwdOjTTXl5eHpWVlbFz5856n68hbaHGqqqqKC4ujqlTp2baKyoqMs/f1ONdW2uN29Bc1nQwjNvQ\n",
              "udOQtlJja57fNR3IuDUdDPubq/eR2jZv3tzmavRe1zLjRkSUlJTEAw88ELt3745BgwbFokWL4rzz\n",
              "zovaTjzxxFi1alVERNxzzz0xZsyYWLlyZZ1+9dm7d29885vfbLDPscceG08++WSMHDkyIiJmz54d\n",
              "gwYNavQY9bnuuuv+6ucAANoHt18BAO3GZz/72YiISKfT8b//+79ZVwvVp6SkJNauXRsRET//+c9j\n",
              "wIAB0b9//zjnnHNizZo1EfF+KHXKKafE2LFjo6ioKP7rv/4rSktLG7ziZsyYMfHQQw9FRMTWrVtj\n",
              "+fLlUVJSktm+b9++uO222+LUU0+NU089Nb70pS/F7t27I+L9kGr8+PExbNiwOOGEE+KSSy7JbJs6\n",
              "dWrcfPPN9Y5bVVUV27Zty/ral67+0HkAAJJHqAMAtCv79u2Lq666Kv7u7/4uzj777A/t/9hjj8XA\n",
              "gQPjnXfeic9//vPx8MMPxyuvvBLjxo2LUaNGRTqdjoiI3/72t3H11VfHqlWr4tJLL/3Q5/37v//7\n",
              "qKioiA0bNsSjjz4al156aeTn52e2z5o1K1asWBErV66MVatWxRtvvBFlZWWZ7atWrYqf/OQn8dvf\n",
              "/jb+/Oc/17k6qT7Tpk2Lbt26ZX2t+t8/NuqxAECyCHUAgHbl/vvvj82bN+/3VsIPrF27NoqKiqKo\n",
              "qChef/31ePjhh+PFF1+Mfv36Rb9+/SIi4sorr4wNGzbE22+/HRHvr9tzzjnnNKmWf/zHf4zy8vJ4\n",
              "6KGH4pprrsnatmjRoigtLY2CgoLo0KFDXHvttfGLX/wis/2zn/1sdOrUKfLz8+P000+PN954o1Fj\n",
              "Tp48ObZu3Zr1VfSRjzWpbgAgGVp0TR0OzAd/EYS2oPYaGNAWFRYW7neRZA5Ov/71r+PCCy+MDh3q\n",
              "/zWn5po6H3j11VcbfN7ai4U3xtVXXx0DBgyIE044IY4//vgG+9Ze0Lljx46Zf+fn58fevXsbNWZB\n",
              "QUGd287yU/6OBwDtkZ/wAEC7cu2112Z9ylZjnXHGGfHqq6/Ga6+9FhHv35bVq1ev6NWr1wHXcvTR\n",
              "R8e0adPirrvuqrNt+PDh8cgjj8Tu3btj79698eCDD8anPvWpAx4LADj4CHXaoKKiojqflAO5csEF\n",
              "F2QWEIW2asOGDXHWWWflugzaiJ/+9KexYsWKJj/uox/9aPzoRz+Kq6++Ovr37x/33Xdf/Od//med\n",
              "K2iaasyYMTF48OA67ePGjYsBAwbEgAEDoqioKAoLCxtcABkAoDa3X7VBtS8Hh1x66qmncl0CfKij\n",
              "jz46nnvuuVyXQRvxYR81PnTo0Hp/1paUlGR9QlVDj2lozZ76bl2t+fHt+fn5MX369Jg+fXqdfrWf\n",
              "u2afms8BABzcWjTUycvLi9LS0qx7wtetWxdjxoyJ1atX1/mlqUOHDpFKpaKsrCwee+yxTPuWLVvi\n",
              "hhtuiPz8/DqP+eCjRhcvXlxn2/4ul26tcfPy8mLSpEnRo0ePTPvGjRtjxowZsXnz5jqPeffdd+vU\n",
              "WltbqDGVSsWcOXNi+fLlmfZdu3ZFcXFxRESTj3dtrTVuQ3NZ08EwbkPnTkPaSo2teX7XdCDj1nQw\n",
              "7G+u3kdqa4s1eq9rmXEBAA42qbRVeQEA2rXxh58ddxx5Ya7LAIB25ag1t+W6BGvqAAAAACSRUAcA\n",
              "AAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQ\n",
              "BwAAACCBOuS6AAAAWlbn0YPiqHtuy3UZAEAzc6UOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAE\n",
              "EuoAAAAAJJBQBwAAACCBhDoAAAAACSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQQEIdAAAA\n",
              "gAQS6gAAAAAkkFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEqhDYzv+93//d4Pb\n",
              "zz777L+6GAAAAAAap9Ghzq233hoREfv27YtVq1ZFnz59IpVKxRtvvBFFRUXx8ssvt1iRAAAAAGRr\n",
              "9O1XK1asiBUrVkRRUVH8/Oc/jz/84Q/x+9//Pp5++ukYMGBAS9YIAAAAQC1NXlPnpZdeivPOOy/z\n",
              "/fDhw2PFihXNWhQAAAAADWtyqJOfnx/PPPNM5vtnn3028vKstwwAAADQmhq9ps4Hvv/978fll18e\n",
              "hxxySERE7N27Nx5//PFmLwwAAACA+jU51DnzzDPjjTfeiNdffz0iIj7+8Y9nAh4AAAAAWscB3Tf1\n",
              "5JNPxk9+8pPo169fbNq0KV599dXmrgsAAACABjQ51PnGN74RDz74YJSXl0dERCqVivHjxzd3XQAA\n",
              "AAA0oMmhzo9//ONYsGBBdO7cOSIievbsGdu3b2/2wgAAAACoX5NDnY985CORn5+f1ZZOp5utIAAA\n",
              "AAA+XJMXSu7du3c899xzkUqlYs+ePfGtb30rioqKWqA0AAAAAOrT5FBnxowZMXr06Hj11Vejc+fO\n",
              "ce6558aPfvSjlqgNAAAAgHo0OdQ58sgjY+HChbFz585Ip9OZtXUAAAAAaD1NXlPnJz/5SWzbti06\n",
              "deoU9913X4waNSpee+21lqgNAAAAgHo0OdT5+te/Hl27do3f/OY38cMf/jDOO++8+OIXv9gStQEA\n",
              "AABQjyaHOh06vH/H1tNPPx3jxo2L8ePHx44dO5q9MAAAAADq1+RQZ9++ffHiiy/GvHnz4txzz42I\n",
              "iD179jR7YQAAAADUr8kLJd95550xfvz4GD58eJx00kmxdu3aOOGEE1qiNgAAmsF/L1kXX5mwINdl\n",
              "AEC78u2yEbkuIVLpdDqd6yIAAGg5gwZcHMXnfCHXZQBAu9IWQp0mX6kTEfGrX/0qVq1aFbt27cq0\n",
              "3XTTTc1WFAAAAAANa3Ko861vfSvmzp0bb731Vpxzzjnxi1/8IoYNGybUAQAAAGhFTV4oec6cOfHC\n",
              "Cy/EMcccE/PmzYsVK1ZEXl6TnwYAAACAv0KT05iOHTtGx44do7q6OtLpdJx44onxxhtvtERtAAAA\n",
              "ANSjybdffeQjH4k9e/ZEUVFRTJw4MY455pjYt29fS9QGAAAAQD2afKXOfffdF7t374677747tm3b\n",
              "Fs8//3z84Ac/aInaAAAAAKhHk6/UOfXUUyMionPnzvFv//ZvzV4QAAAAAB+u0aHO3XffHbfeemtM\n",
              "mDAhUqlUne333HNPsxYGAAAAQP0aHep06dIlIiK6d+/eUrUAAAAA0EiNDnXGjx8fERFTpkxpsWIA\n",
              "AAAAaJwmL5S8ZcuWuO6666Jv375x/PHHx/XXXx9btmxpidoAAAAAqEeTQ53S0tLIz8+PefPmxdy5\n",
              "cyM/Pz9KS0tboDQAAAAA6tPkT7/63e9+Fz/+8Y8z33/ve9+Lk046qVmLAgAAAKBhTb5S5+ijj45N\n",
              "mzZlvt+0aVP06tWrWYsCAAAAoGGNvlLnlltuiYiIHj16RL9+/eIf/uEfIiLiqaeeirPOOqtO/1mz\n",
              "ZsXcuXMjL+//cqOdO3fG448/HjfeeGPs2LEjq3+fPn3i3nvvzWq7/vrrY926dVltnTt3jpkzZ8Zl\n",
              "l10WnTp1yrRXV1fHqFGjYty4cVFeXh7z58+P+fPn16nrjjvuiGXLlmW1bd++PZYuXRpDhgzJfMrX\n",
              "BwYPHhwDBw6MsrKyOOSQQzLtVVVVMX369Jg9e3aTa4yInMxNksYdO3ZsTJw4MQoKCjLte/bsiQkT\n",
              "JsTKlSubfAxrL/A9cuTIZh23pgM595t7f5v7nB03blxW/+Ycd968eVltzf0aHThwYKZt5cqVOZ/n\n",
              "lt7ftjZurs6r1nxPGjFiRKZt48aN7X5c78Ft6/eGnj17BgBw8Gp0qNOtW7eIiOjXr1/069cv037d\n",
              "ddftt/+GDRti1qxZUVhYmGmbOnVqVFVVxWGHHVbnPxj7W5dn586dsXDhwjr9qqqqori4OKZOnZpp\n",
              "r6ioiPLy8g/dj/Xr1+/3OSMi+vbtW+c5SktLo3fv3nH77bfH0KFDM+3l5eVRWVl5wDXmam6SMm5l\n",
              "ZWWMHj06q4YlS5ZERUXFAR3D2pp73JoO5Nxv7v1tiXO2puYct7bm3t+a2sI8t/T+trVxc3VeteZ7\n",
              "Uk0Hw7jeg9vW7w37U1VVVWdburp6v30BgGRrdKjjo8wBANq+adOmxR133JHV1vOoE3NUDQDQkpq8\n",
              "ps4f//jHGDFiRBQVFUVExKpVq6KsrKy56wIA4ABMnjw5tm7dmvXV88gTcl0WANACmhzqjB8/Pi6/\n",
              "/PJIp9MREXHqqafGQw891OyFAQDQdAUFBdG1a9esr1Rek3/lAwASoMk/4d9555246qqrMov1dejQ\n",
              "ITp0aPIno7eo0tLS/S6SDAAAANBeNDnU6dChQ+YqnYiILVu2ZH0PAAAAQMtrcqhz6aWXxvjx42Pb\n",
              "tm3x4IMPxnnnnRdf+MIXWqK2A/bkk0/GhAkTcl0GAAAAQItp8n1Tt956azz66KOxdevWePrpp+OW\n",
              "W26Jz3/+8y1R2wG76KKL4qKLLsp1GQAAAAAt5oAWw7niiiviiiuuaLBPXl5elJaWRseOHTNt69at\n",
              "izFjxsTq1aujpKQku5D9rMuzefPmOv3efffdSKVSMWfOnFi+fHmmfdeuXVFcXPyhtefn59d5zjVr\n",
              "1kRExOLFi+ts69WrV+Tl5cWkSZOiR48emfaNGzfGjBkzDrjGXM1NUsZNpVJRVlYWjz32WKZ9y5Yt\n",
              "ccMNNxzQMaytucet6UDO/ebe35Y4Z2tqznFra+79raktzHNL729bGzdX51VrvifVdDCM6z24bf3e\n",
              "AAAc3FLpRi6IM2bMmEilUvVu9wlYAABt06ABF0fxOW3rdnkASLpvl43IdQmNX1Nn0KBBMXDgwDj0\n",
              "0ENj+fLl0adPnzjuuOPiV7/6VRQUFLRkjQAAAADU0ujbrz641Pvss8+O5cuXR9euXSMi4ktf+lKM\n",
              "GJH7dAoAAADgYNLkT7/atGlTJtCJiOjatWts2rSpWYsCAAAAoGFNXij5tNNOi9LS0hg7dmxERPz7\n",
              "v/97nHbaac1eGAAAAAD1a/KVOg8++GAcccQRcfPNN8fNN98cH/3oR+PBBx9sidoAAAAAqEeTr9Tp\n",
              "0qVLfPvb3653+3e/+9348pe//FcVBQAAAEDDmnylzod5+OGHm/spAQAAAKil2UOddDrd3E8JAAAA\n",
              "QC3NHuqkUqnmfkoAAAAAamn2UAcAAACAltfkhZI/jNuvAADalrOH9olv3zMi12UAAM3sgK7U2bhx\n",
              "YyxZsiQiIvbu3Ru7d+/ObCsvL2+OugAAAABoQJNDnblz58YZZ5wRpaWlERGxevXquPjiizPbTzvt\n",
              "tOaqDQAAAIB6NDnUmTZtWrz88svRo0ePiHg/xHnzzTebvTAAAAAA6tfkUCc/Pz8OP/zwrLZDDz20\n",
              "2QoCAAAA4MM1OdQ57LDD4s9//nPmo8t/+ctfxt/8zd80e2EAAAAA1K/Jn3511113xfnnnx/r1q2L\n",
              "IUOGxPr16+OnP/1pS9QGAAAAQD2aHOoMGjQonnnmmXjhhRcinU7HmWeeGd27d2+B0gAAAACoT5ND\n",
              "nYiIbt26xfnnn9/ctQAAAADQSI0OdXr06JFZR6emdDodqVQq/vKXvzRrYQAAAADUr9GhzqpVq1qw\n",
              "DAAAAACaotGfftW7d+/o3bt3/OAHP8j8u2YbAAAAAK2nyR9p/sQTTzSqDQAAAICW0+jbr37+85/H\n",
              "woUL4+23345bbrkl075169YWKQwAAACA+jU61OnYsWN079498vLyolu3bpn2j33sY3H77be3SHEA\n",
              "AAAA7F+jQ51zzjknzjnnnLj44ovjtNNOa8maAAAAAPgQjQ51Hn300bjiiivi2WefjWeffbbO9ptu\n",
              "uqlZCwMAAACgfo0OdV5//fWIeH9tnSOOOCJr26ZNm4Q6AAAAAK2o0aHOHXfcERERGzdujJ/+9KdZ\n",
              "2wYMGNC8VQEAAADQoEaHOrt3745du3bFvn37orKyMtLpdES8/+lXO3bsaLECAQAAAKgrr7Edp02b\n",
              "Ft27d4/XXnstunXrFt27d4/u3btHv3794qqrrmrJGgEAAACopdGhzpQpU6K6ujrGjRsX1dXVma/3\n",
              "3nvPR5oDAAAAtLJGhzofuO+++1qiDgAAAACaoMmhDgAAAAC5J9QBAAAASCChDgAAAEACCXUAAAAA\n",
              "EkioAwAAAJBAQh0AAACABBLqAAAAACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAIJdQAA\n",
              "AAASSKgDAAAAkEBCHQAAAIAESqXT6XSuiwAAoOWMOSUVkz/pb3kA0JxOKN+X6xJcqQMAAACQREId\n",
              "AAAAgAQS6gAAAAAkkFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAAAJBA\n",
              "Qh0AAACABBLqAAAAACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAJ1aI1BZs2aFXPnzo28\n",
              "vP/LkHbu3BmPP/543HjjjbFjx46s/n369ImxY8fGxIkTo6CgINO+Z8+emDBhQqxcuTKWLVuW9Zjt\n",
              "27fH0qVLY8iQIdGlS5esbYMHD46BAwdGWVlZHHLIIZn2qqqqmD59esyePTvWrVuX9ZjOnTvHvHnz\n",
              "IiIilUrFli1bonv37ll9Vq5cmfMaZ86cGZdddll06tQp015dXR2jRo2KcePGZfVvrXEjolmP94gR\n",
              "IzJtGzdubLVxm/sYDhw4MNPW3OfOlClTstpGjhyZ8/1t7+fsvffem9V2/fXX5/y10tAxrKk135Mj\n",
              "IgoLC2P+/PlRVFSU1SfJr2fvjW3rPGtr78mtee707NkzAICDV6uEOhs2bIhZs2ZFYWFhpm3q1KlR\n",
              "VVUVhx12WCY8+UBpaWlUVlbG6NGjo7S0NNO+ZMmSqKioiPXr18fChQvrPCYiom/fvlFeXl5nW+/e\n",
              "veP222+PoUOHZtrLy8ujsrIydu7cWe/zNaQt1FhVVRXFxcUxderUTHtFRUWd52/tcZvzeNfUmuM2\n",
              "9zGsqbnHra0t7G97P2drawuvlYaOYU2t+Z7ckLYyN235PEvae2NNbf1nf01JPnf2p6qqqs62fdX7\n",
              "7QoAJJzbrwAA2pFp06ZFt27dsr5e2ZzrqgCAliDUAQBoRyZPnhxbt27N+ur//3JdFQDQElrl9isA\n",
              "AFpHQUFB1vpAERH5/owHAO2SUKcR0ul0rksASKza678AAADNw99tAAAAABJIqNMIRUVFdT4pA4DG\n",
              "ueCCC2Lt2rW5LgMAANodt181wqpVq3JdAkBiPfXUU7kuAQAA2qVWCXXy8vKitLQ0OnbsmGlbt25d\n",
              "jBkzJlavXh0lJSXZRXXoEKlUKsrKyuKxxx7LtG/ZsiVuuOGGyM/Pr/OYNWvWRETE4sWL62zr1atX\n",
              "5OXlxaRJk6JHjx6Z9o0bN8aMGTNi8+bNdR7z7rvvfuh+tYUaU6lUzJkzJ5YvX55p37VrVxQXF9ep\n",
              "tzXHbc7jXVNrjtvcx7D2fjTnuLW1hf1t7+dsbW3htdLQMaypNd+TG9JW5qYtn2dJe2+sqa3/7K8p\n",
              "yecOAHBwS6WtAgwA0K6NOSUVkz/prnsAaE4nlO/LdQnW1AEAAABIIqEOAAAAQAIJdQAAAAASSKgD\n",
              "AAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCBhDoAAAAACSTUAQAAAEggoQ4AAABAAnXIdQEAALSs\n",
              "Hp+eECfcc0+uywAAmpkrdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCBhDoAAAAA\n",
              "CSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQQEIdAAAAgAQS6gAAAAAkkFAHAAAAIIGEOgAA\n",
              "AAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAAAJBAQh0AAACABBLqAAAAACSQUAcAAAAggYQ6\n",
              "AAAAAAkk1AEAAABIIKEOAAAAQAIJdQAAAAASSKgDAAAAkEBCHQAAAIAEEuoAAAAAJJBQBwAAACCB\n",
              "hDoAAAAACSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQQEIdAAAAgAQS6gAAAAAkUCqdTqdz\n",
              "XQQAAC2ny6c/Gd0vH5brMgCgXfmfMf+S6xJcqQMAAACQREIdAAAAgAQS6gAAAAAkkFAHAAAAIIGE\n",
              "OgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAAAJBAQh0AAACABBLqAAAAACSQUAcAAAAg\n",
              "gYQ6AAAAAAkk1AEAAABIIKEOAAAAQAJ1yHUBrW3WrFkxd+7cyMv7vzxr586d8fjjj8eNN94YO3bs\n",
              "yOrfp0+fuPfee7Parr/++li3bl1WW+fOnWPmzJlx2WWXRadOnTLt1dXVMWrUqIiIJo87duzYmDhx\n",
              "YhQUFGTa9+zZExMmTIgRI0Zk2jZu3Nhq465cuTKWLVuW9Zjt27fH0qVLs9oOZJ4/bH+nTp0a7733\n",
              "XnznO9+J2uo7JvPmzctqu+OOO+qtf8iQIdGlS5esbYMHD46BAwdGWVlZHHLIIZn2qqqqmD59esye\n",
              "PbvJ58K4ceOy+jfnuAfb/rb2a+5AzsHWrLGh12d98z5lypSstpEjR7bKuO3lPGuL53fPnj0zbQsW\n",
              "LMj566r2z9CKioo47rjjol+/fpFOp6NDhw4xffr0OPfcc6Mp7r///qisrIzbbrutzrahQ4fGCy+8\n",
              "EP/zP/8TRxxxRERErFu3Lvr27RsXXXRRzJ8/v0lj1fTkk0/GM888E2VlZXW2lZaWxpw5c+L111+P\n",
              "Pn36RETExIkTo0uXLjF16tQDHhMAoD4HXaizYcOGmDVrVhQWFmbapk6dGlVVVXHYYYfV+U9xaWlp\n",
              "nefYuXNnLFy4sE6/qqqqKC4uzvrFraKiIsrLyyMimjxuZWVljB49OquGJUuWREVFRVbf1hx3/fr1\n",
              "+9332g5knhu7v/tT3zGpraH6+/btm5mzmtt69+4dt99+ewwdOjTTXl5eHpWVlQd8LtTUnOMebPub\n",
              "q9fc/rSFGg/keNfWWuO2p/MsV+PWd+7UtHnz5py/rvbnsMMOi1WrVkVExBNPPBGf+9zn4p133olU\n",
              "KrXf/rXt3bs3rrvuugb79O/fP37wgx/ErbfeGhERDz30UAwcOLBRz9+Qiy66KC666KJ6t/fq1Su+\n",
              "/vWvx6OPPvpXj3Wgqqqq6pwLsS+dm2IAgBbl9isAIGdKSkpi8+bN8e6778ZLL70UZ555ZvTv3z9O\n",
              "P/30eP755yPi/VCpe/fuMWnSpBgwYEDMnDkzpk6dGjfffHO9zzt69Oh4+OGHI+L9K40ef/zx+Pzn\n",
              "P5/V51//9V/jlFNOiX79+sWVV14ZW7dujYj3g6rLLrssLrzwwjj55JOjuLg4/vKXv0TE+6HYxRdf\n",
              "XO+448aNi+effz5efvnl/W6fPn16nH766TFgwIAoKSmJN998MyLeD4M2bNgQERGf+9zn4swzz4yI\n",
              "9wOaww8/PKqqqmL58uUxcODAKCoqilNPPTXuu+++/Y4xbdq06NatW9bX7vUb660ZAEguoQ4AkDOP\n",
              "PvpoHHvssdG1a9e45JJLYsqUKfHKK6/EPffcEyNHjozt27dHRMTWrVvjlFNOiZdffrnBMOcDH/vY\n",
              "x+Koo46KF198MZ5++ukYNGhQ9OjRI7P9Zz/7WTz00EPx/PPPx6uvvhqdO3eOr371q5ntL774YpSX\n",
              "l8eaNWviiCOOiAceeKBR+/ORj3wkpkyZEpMmTaqzbc6cObF27dpYtmxZvPzyy3HllVfG9ddfHxER\n",
              "w4YNi0WLFkV1dXX85je/ia1bt8a2bdti6dKlMXDgwCgoKIhp06bFxIkTY9WqVfHaa6/F5Zdfvt8a\n",
              "Jk+eHFu3bs36OvRve+63LwCQbAfd7VcAQG5VVlZGUVFRRLx/hcqTTz4Za9eujby8vPj0pz8dEe+v\n",
              "T3TkkUfGqlWr4phjjolDDjkkrrrqqiaNc80118Ts2bNjy5YtMW7cuHj77bcz2xYtWhSXXXZZdO/e\n",
              "PSIivvjFL8all16a2V5SUhKHH354RLy/LtKrr77a6HFLS0vj7rvvjl/84hdZ7fPnz48VK1ZkbgPb\n",
              "t29fZtvw4cNj0aJFccopp8Rpp50WRx55ZCxZsiSWLVsWw4YNi4iIc889N/75n/85fv/730dxcXEM\n",
              "GTJkv+MXFBRkrX8VERH5jbu1DQBIFqEOiWGRSXLNOQjNo+aaOh/YX2hSc42dTp06ZS3Q3BgXX3xx\n",
              "TJo0KQoKCmLYsGHxyCOP1Nu39no+HTt2zPw7Pz8/9u7d2+hx8/Pz41vf+lZ89atfzVrPKJ1Ox+TJ\n",
              "k+sssB3xfqgzefLkOPnkk2P48OFx5JFHxqJFi2LZsmWZ26xuvvnm+MxnPhOLFi2Kr33ta3HqqafW\n",
              "WYgaADi4uP0KAMi5E088MaqrqzNXt7zwwgvxpz/9KXNFz4Ho2LFjlJWVxYwZM+oEQsOHD4//+I//\n",
              "iG3btkVExAMPPBCf+tSnDnis2i6++OIoKCiIJ554Iqvt/vvvz6zPs2fPnvj1r38dERFHH310dOvW\n",
              "Le6///4YPnx4nHvuubFgwYKoqKiIAQMGRETE2rVr42//9m/j2muvja997WuxfPnyZqsXAEgmV+qQ\n",
              "GA19fC20BucgtJxDDz00nnjiibjpppvi1ltvjY4dO8bcuXOjS5cusXnz5gN+3ksuuWS/7eeff368\n",
              "9tprMXjw4MjLy4v+/fs3+1Uvd911V5x99tmZ76+88sp49913Mx/fvnfv3rjmmmviE5/4RES8HzQt\n",
              "WLAg83HoRx11VHziE5/IBFIzZ86MxYsXx6GHHhr5+flx9913N2u9AEDyCHVIjA/7+Fpoac5B+OsV\n",
              "FhbGe++9t99tgwYNihdeeKFRj2nodsglS5bst720tDTrY9Zvu+22/Ya0tZ/7xhtvrPc5aqr9cfZn\n",
              "nXVWpNPZHyV+0003xU033bTfx8+YMSNmzJiR+b72XHzve9/b7+MAgIPXQRfq5OXlRWlpada98uvW\n",
              "rYsxY8bE6tWro6SkJKt/hw51p2jz5s11+r377ruRSqVizpw5WZdD79q1K4qLiyMimjxuKpWKsrKy\n",
              "eOyxxzLtW7ZsiRtuuCGrb2uOm5+fX+cxa9asqTNHBzLPjd3f/anvmNTWUP2LFy+us61Xr16Rl5cX\n",
              "kyZNyvrUlI0bN8aMGTMO+FyoqTnHPdj2N1evuf1pCzUeyPGurbXGbU/nWa7Gre/cqaktvK4AAGg5\n",
              "qXTtPyEBANCudPn0J6P75cNyXQYAtCv/M+Zfcl2ChZIBAAAAkkioAwAAAJBAQh0AAACABBLqAAAA\n",
              "ACSQUAcAAAAggYQ6AAAAAAkk1AEAAABIIKEOAAAAQAIJdQAAAAASSKgDAAAAkEAdcl0AAAAta9wp\n",
              "Z8U9Y/4l12UAAM3MlToAAAAACSTUAQAAAEggoQ4AAABAAgl1AAAAABJIqAMAAACQQEIdAAAAgAQS\n",
              "6gAAAAAkkFAHAAAAIIGEOgAAAAAJJNQBAAAASCChDgAAAEACCXUAAAAAEkioAwAAAJBAQh0AAACA\n",
              "BBLqAAAAACSQUAcAAAAggYQ6AAAAAAnUIdcFAADQcqqqquJnP/tZ7Nu3L/Lz83NdzkFj37598atf\n",
              "/SpOP/10897KzH1umPfcMfe50Rrz3rt37/jyl7/cYJ9UOp1Ot8joAADk3LZt26Jbt26xdevW6Nq1\n",
              "a67LOWiY99wx97lh3nPH3OdGW5l3t18BAAAAJJBQBwAAACCBhDoAAAAACSTUAQBoxwoKCmLKlClR\n",
              "UFCQ61IOKuY9d8x9bpj33DH3udFW5t1CyQAAAAAJ5EodAAAAgAQS6gAAAAAkkFAHAAAAIIGEOgAA\n",
              "7cDvf//7OPPMM+OEE06IT37yk7F69er99ps9e3Ycf/zxcdxxx8W1114be/bsaeVK25fGzPvixYvj\n",
              "9NNPj5NPPjlOOeWU+MpXvhLV1dU5qLb9aOz5HhGRTqejuLg4unfv3noFtmONnftXX301hg4dGied\n",
              "dFKcdNJJ8cQTT7Rype1LY+a9uro6brnlljj55JOjf//+ce6558Yf/vCHHFTbftx0001RWFgYqVQq\n",
              "Vq1aVW+/XP5sFeoAALQD48ePj3HjxsXvfve7mDRpUpSWltbps379+rj99tvjueeeiz/84Q/x5z//\n",
              "OWbNmtX6xbYjjZn3Hj16xGOPPRZr1qyJlStXxgsvvBCPPPJI6xfbjjRm3j9QVlYWxx13XOsV1841\n",
              "Zu537twZn/nMZ+LOO++M3/72t/Haa6/FWWed1frFtiONmfcnn3wynn/++fjNb34Tr7zySgwbNiy+\n",
              "9rWvtX6x7cioUaNi6dKl0bt373r75Ppnq1AHACDh3nnnnXjppZfiqquuioiIkSNHxh//+Mc6f6Gd\n",
              "O3duXHTRRXHUUUdFKpWK6667Lh599NFclNwuNHbeP/GJT0SfPn0iIqJjx45RVFQUFRUVrV1uu9HY\n",
              "eY+IWL16dcyfPz+++tWvtnaZ7VJj537OnDlxxhlnxJAhQyIiIj8/Pz760Y+2er3tRWPnPZVKRVVV\n",
              "VezatSvS6XRs27YtjjnmmFyU3G6cffbZHzqHuf7ZKtQBAEi4P/7xj9GzZ8/o0KFDRLz/i/2xxx4b\n",
              "b731Vla/t956K+uvjYWFhXX60HiNnfea/vSnP8XcuXNjxIgRrVVmu9PYed+zZ09ce+218cADD0R+\n",
              "fn4uSm13Gjv3a9asiYKCghgxYkQUFRXF1VdfHZs2bcpFye1CY+f9wgsvjKFDh8ZRRx0VPXv2jF/+\n",
              "8pfxzW9+MxclH1Ry/bNVqAMAAK1g27ZtceGFF8ZXvvKVGDRoUK7LaffuuOOOuOSSS+Kkk07KdSkH\n",
              "nb1798aiRYvigQceiF//+tfRq1ev+OIXv5jrstq9l156KV577bV4++23Y8OGDTFs2LC47rrrcl0W\n",
              "LUyoAwCQcB/72Mdi48aNsXfv3oh4f2HYt956K4499tisfscee2y8+eabme8rKirq9KHxGjvvERGV\n",
              "lZVRUlISn/nMZ+KWW25p7VLblcbO+7PPPhvf+973orCwMIYMGRLbtm2LwsJCV4z8FZryXnPuuedG\n",
              "r169IpVKxVVXXRXLly/PRcntQmPn/ZFHHsksCp6XlxejR4+OZ555JhclH1Ry/bNVqAMAkHBHHHFE\n",
              "DBgwIH74wx9GRMS8efPimGOOib59+2b1GzlyZDz55JPxpz/9KdLpdNx///1x+eWX56LkdqGx8759\n",
              "+/YoKSmJkpKS+Kd/+qdclNquNHben3vuuXjzzTejoqIili5dGl27do2Kigpru/wVGjv3n/vc52LF\n",
              "ihWxbdu2iIh46qmn4rTTTmv1etuLxs57nz59YvHixbF79+6IiFiwYEGceuqprV7vwSbnP1vTAAAk\n",
              "3uuvv54+44wz0scff3x64MCB6VdeeSWdTqfTY8eOTf/4xz/O9Js1a1a6T58+6T59+qSvueaa9O7d\n",
              "u3NVcrvQmHm/88470x06dEifdtppma8777wzl2UnXmPP9w+sX78+3a1bt1ausn1q7Nw/8sgj6VNO\n",
              "OSXdr1+/dElJSfqtt97KVcntQmPmfdeuXekvfOEL6Y9//OPpfv36pc8777z0G2+8kcuyE2/cuHHp\n",
              "Xr16pfPz89NHHHFE+rjjjkun023rZ2sqnU6nWy9CAgAAAKA5uP0KAAAAIIGEOgAAAAAJJNQBAAAA\n",
              "SCChDgAAAEACCXUAAAAAEkioAwAAAJBAQh0AAACABBLqAAAAACSQUAcAAAAggYQ6AAAAAAkk1AEA\n",
              "AABIoP8PmdhIZpN1//UAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-b7ddb3a0-b9f1-4586-af99-0c69894ea06e\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-b7ddb3a0-b9f1-4586-af99-0c69894ea06e\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5 — Human-only splits + stage pools + scaler (fit on Stage-1 only)"
      ],
      "metadata": {
        "id": "bOIiCKD5gHHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Human-only rows\n",
        "is_human = (df_all[\"label_source\"] == \"human\")\n",
        "df_human = df_all[is_human].copy()\n",
        "assert len(df_human) > 0, \"No human-labeled rows found.\"\n",
        "\n",
        "# Stratified 70/15/15 on human labels\n",
        "idx = np.arange(len(df_human))\n",
        "h_tr_idx, h_tmp = train_test_split(idx, test_size=0.30, stratify=df_human[\"label\"].values, random_state=42)\n",
        "h_va_idx, h_te_idx = train_test_split(h_tmp, test_size=0.50, stratify=df_human[\"label\"].values[h_tmp], random_state=42)\n",
        "\n",
        "df_h_tr = df_human.iloc[h_tr_idx].copy()\n",
        "df_h_va = df_human.iloc[h_va_idx].copy()\n",
        "df_h_te = df_human.iloc[h_te_idx].copy()\n",
        "\n",
        "print(\"Human splits sizes:\", len(df_h_tr), len(df_h_va), len(df_h_te))\n",
        "\n",
        "# Exclude human val/test rows from Stage-1 to avoid leakage\n",
        "human_valtest_ids = set(df_h_va.index.tolist() + df_h_te.index.tolist())\n",
        "mask_stage1 = ~df_all.index.isin(human_valtest_ids)\n",
        "\n",
        "df_stage1 = df_all[mask_stage1].copy()     # Stage-1: human-train + AI + auto\n",
        "df_stage2 = df_h_tr.copy()                  # Stage-2: human-train\n",
        "df_eval_val = df_h_va.copy()                # Validation on human-val\n",
        "df_eval_test = df_h_te.copy()               # Final test on human-test\n",
        "\n",
        "# Source-based sample weights (Stage-1 only)\n",
        "weight_map = {\"human\": 1.0, \"ai\": 0.7, \"auto\": 0.5}\n",
        "df_stage1[\"sample_weight\"] = df_stage1[\"label_source\"].map(weight_map).fillna(0.5).astype(\"float32\")\n",
        "\n",
        "print(\"\\nStage-1 size:\", len(df_stage1), \"(by source)\")\n",
        "print(df_stage1[\"label_source\"].value_counts())\n",
        "print(\"\\nStage-2 (human-train) size:\", len(df_stage2))\n",
        "print(\"\\nEval val/test sizes:\", len(df_eval_val), len(df_eval_test))\n",
        "\n",
        "# ----- Fit scaler on Stage-1 only (no leakage) -----\n",
        "scaler = StandardScaler()\n",
        "X1 = scaler.fit_transform(df_stage1[NUM_FEATS].values.astype(\"float32\"))\n",
        "X2 = scaler.transform(df_stage2[NUM_FEATS].values.astype(\"float32\"))\n",
        "XV = scaler.transform(df_eval_val[NUM_FEATS].values.astype(\"float32\"))\n",
        "XT = scaler.transform(df_eval_test[NUM_FEATS].values.astype(\"float32\"))\n",
        "\n",
        "# Stash arrays for dataset builders\n",
        "df_stage1[\"_eng_feats_np\"]   = list(X1)\n",
        "df_stage2[\"_eng_feats_np\"]   = list(X2)\n",
        "df_eval_val[\"_eng_feats_np\"] = list(XV)\n",
        "df_eval_test[\"_eng_feats_np\"] = list(XT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fthegML6gOpb",
        "outputId": "23d54fce-7da4-4861-de48-0adf9d843921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human splits sizes: 7000 1500 1500\n",
            "\n",
            "Stage-1 size: 250070  (by source)\n",
            "label_source\n",
            "ai       243070\n",
            "human      7000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Stage-2 (human-train) size: 7000\n",
            "\n",
            "Eval val/test sizes: 1500 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6 — Tokenize (title + description) and attach numeric features"
      ],
      "metadata": {
        "id": "xWP5i2IzgoJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "\n",
        "MODEL_NAME = \"csebuetnlp/banglabert\"  # or \"sagorsarker/bangla-bert-base\"\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "MAX_LEN = 192\n",
        "TRUNCATION_POLICY = \"only_second\"  # keep title fully; truncate description if needed\n",
        "\n",
        "def map_pair_with_numeric(df, include_weights=False):\n",
        "    enc = tok(\n",
        "        text=df[TITLE_COL].astype(str).tolist(),\n",
        "        text_pair=df[DESC_COL].astype(str).tolist(),\n",
        "        truncation=TRUNCATION_POLICY,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "    data = {\n",
        "        \"input_ids\": enc[\"input_ids\"],\n",
        "        \"attention_mask\": enc[\"attention_mask\"],\n",
        "        \"labels\": df[\"label\"].astype(int).tolist(),\n",
        "        \"eng_feats\": np.stack(df[\"_eng_feats_np\"].values).astype(\"float32\"),\n",
        "    }\n",
        "    if \"token_type_ids\" in enc:  # if model uses segment ids\n",
        "        data[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
        "    if include_weights:\n",
        "        data[\"sample_weight\"] = df[\"sample_weight\"].astype(\"float32\").tolist()\n",
        "\n",
        "    ds = Dataset.from_dict(data)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds\n",
        "\n",
        "ds_stage1 = map_pair_with_numeric(df_stage1, include_weights=True)\n",
        "ds_stage2 = map_pair_with_numeric(df_stage2, include_weights=False)\n",
        "ds_val    = map_pair_with_numeric(df_eval_val, include_weights=False)\n",
        "ds_test   = map_pair_with_numeric(df_eval_test, include_weights=False)\n",
        "\n",
        "len(ds_stage1), len(ds_stage2), len(ds_val), len(ds_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "1579a77e3eb148069dcc6ee205c396b1",
            "f821cd7ef62a4087a3dc38b47e6f93a8",
            "8248aa54ce614be3927e293cfa450bd9",
            "e904dec339a041e7b7eea8ba8dc3cb4f",
            "4ce954a5d2e440f3886388271d935654",
            "9c2294d593624994afe29d14bba58b24",
            "c1cfde57168f43e18f35f7be3abd8d3a",
            "3ec955326cec45df9746e675700a3c7f",
            "0be0f9e12773474097e51f3dfe01b91b",
            "69718da4f6e24523a625624d61488895",
            "6aedb29bfc0b4734a834dfdd605b07a5",
            "5fd86097962748388ebf525988fda043",
            "beac77870f9046c5a3d1d0165e53959f",
            "9ddf40a9849a42b28c10b74505e5d588",
            "560f641b66684e9c8bc3963494d3505c",
            "b2f67068408346459fdbad144ddcc011",
            "9b4746c7c9914858a14a69642b5e618f",
            "8ae248a011924b298e0c6ccbca407c11",
            "e3df10fd4a0d44768138a70dc93dbbdf",
            "691b881b25c24e1eb1ce22215ce05f97",
            "4126572222824ab98d0bbd7acdf7d1cf",
            "5e1f52dbf3174a098f3031b256dba7e9",
            "46e91bfbfc9f4806bff621863858e507",
            "cbf2b5da475f4435b9e17f88d05d3330",
            "a7e67fadd41848999f03507444094064",
            "754d0d9d032b4e82b54b01621b3a36f7",
            "bf8e7a1cb1894514aa772b2029a60d61",
            "e2119738b2bd4a718f5dd2aaeaa1b87f",
            "774ba1303b9949c984590fa3457d14f0",
            "950414f878374cc39b7ee354c82e1d78",
            "740674838b3f4f7b96e1a446a1acb0a1",
            "0699cccec09342be947904e8ed3ac9a7",
            "f537892e1b0e44f8b1c70e8cabb2a5df",
            "54c022d07c8645b0a5005e89d348801c",
            "daf7fc89ca0f44928f20960314aa0d7b",
            "9c3d31db6a2a4011b07a1a28ee08d876",
            "b9011aa300324bb69192731879a0327f",
            "f58c09d5778b4b8d9c8436839da3ebe7",
            "cdd81a99cd7048378a605d19d4be3a07",
            "ba33cade27394fdc94e202861f24ee1a",
            "5b685fc7dd7f43418c754f4c6eac401c",
            "51ebb632e51c407185d349e00691efd8",
            "a8a6d0ab85e34d3a9ed7f9ef3ddd4aea",
            "78c7088cc6184dcf93723a5ec44ebe50"
          ]
        },
        "id": "4l1tBUwIgm6g",
        "outputId": "fef58805-99b1-45ee-f83e-6a73b6187cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1579a77e3eb148069dcc6ee205c396b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fd86097962748388ebf525988fda043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46e91bfbfc9f4806bff621863858e507"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54c022d07c8645b0a5005e89d348801c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250070, 7000, 1500, 1500)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7 — Metrics + Weighted Trainer"
      ],
      "metadata": {
        "id": "5zD2Gi5-gsCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=1)\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(labels, preds)),\n",
        "        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\")),\n",
        "        \"f1_micro\": float(f1_score(labels, preds, average=\"micro\")),\n",
        "        \"kappa\":    float(cohen_kappa_score(labels, preds)),\n",
        "    }\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        weights = inputs.pop(\"sample_weight\", None)\n",
        "\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        ce = nn.CrossEntropyLoss(reduction=\"none\")(logits, labels)\n",
        "        if weights is not None:\n",
        "            weights = weights.to(ce.device).view(-1)\n",
        "            loss = (ce * weights).sum() / (weights.sum() + 1e-8)\n",
        "        else:\n",
        "            loss = ce.mean()\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "8WzynBc9guwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 8 — Custom model: BanglaBERT + small MLP for numeric feats"
      ],
      "metadata": {
        "id": "ibLkjmeRgz_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoConfig\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "class BertWithNumeric(nn.Module):\n",
        "    def __init__(self, model_name: str, num_labels: int = 2, num_numeric: int = 6):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
        "        hidden = self.config.hidden_size\n",
        "\n",
        "        self.num_proj = nn.Sequential(\n",
        "            nn.Linear(num_numeric, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(32)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(getattr(self.config, \"hidden_dropout_prob\", 0.1))\n",
        "        self.classifier = nn.Linear(hidden + 32, num_labels)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        eng_feats=None,\n",
        "        labels=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        bert_out = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        pooled = bert_out.last_hidden_state[:, 0, :]  # CLS\n",
        "\n",
        "        if eng_feats is None:\n",
        "            num_emb = torch.zeros((pooled.size(0), 32), device=pooled.device)\n",
        "        else:\n",
        "            num_emb = self.num_proj(eng_feats)\n",
        "\n",
        "        x = torch.cat([pooled, num_emb], dim=1)\n",
        "        x = self.dropout(x)\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c64a2b4e8fdf4fdf93982b7abb21c318",
            "0b18634a4d1e46ab98b9ecdb2c294b53",
            "662c1d1b01da44dd942f9d68424784e6",
            "3ece667d755549409598dbf4e4e9abc3",
            "37e6aad4326f4d7e9d68c297f7d23c8b",
            "d5a08518c1444df6abdf6fbc53f992d2",
            "ef98be19bf4645eba0de426ae902a355",
            "8249fcb8ea2848da93c277b48f73d8d3",
            "5df44e2deaaa400d8de8def33a48c69d",
            "ad040da9751f44a489de7f7743d3059c",
            "fadd9e38c0b44aa9b27652e59f74b4bb",
            "27d18eb7eda2458ba052f92e33147234",
            "337554f59fa54be0a6eac4e9ceea5bd1",
            "17fc48f298654e148812a53db453b301",
            "180c16e7516a4d1c973461e017dbf6ba",
            "48e76865a8494c3b89ef3a5725688432",
            "7bc561bc8a3a4879959a0a4a2e9278a8",
            "1abd43bd612b43479b533a8dfb4632a3",
            "3823cb9d79d044fdb7c5d35936f39d1e",
            "fdd724fbc9594c5d9521be656dc75843",
            "e7aa9410d60342409ab6475a2456eea6",
            "5eb1b285355a4ddf8621bc60ece83c19"
          ]
        },
        "id": "8FQwKhjYg0_O",
        "outputId": "c4b60093-bdd2-46ce-ae81-666ae10715bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c64a2b4e8fdf4fdf93982b7abb21c318"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27d18eb7eda2458ba052f92e33147234"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15630' max='15630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15630/15630 44:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.368300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.266600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.241900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.235000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.224000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.193600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.197200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.190000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.195000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.187400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.180100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.173000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.179700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.171400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.168900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.167700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.162500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.171400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.163400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.169700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.166500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.160800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.154100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.163200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.164800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.153600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.157600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.160900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.159700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.150300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.160800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.158500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.151700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.147000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.152200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.143100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.146200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.140400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.154500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>0.144800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.135800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>0.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.154200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.164000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.140800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>0.149400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.134800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>0.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.146500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>0.150100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.142900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>0.154300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.142600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.151500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.141400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>0.135700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.139400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.119400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.113700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.117100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.122100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>0.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>0.110200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.120100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>0.116000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.108500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>0.107100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9200</td>\n",
              "      <td>0.108900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9300</td>\n",
              "      <td>0.113100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9400</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.115600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.107900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9700</td>\n",
              "      <td>0.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>0.108800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9900</td>\n",
              "      <td>0.109900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10100</td>\n",
              "      <td>0.103500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10200</td>\n",
              "      <td>0.115700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10300</td>\n",
              "      <td>0.103400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10400</td>\n",
              "      <td>0.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.105300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10600</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10700</td>\n",
              "      <td>0.105600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10800</td>\n",
              "      <td>0.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10900</td>\n",
              "      <td>0.105700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11100</td>\n",
              "      <td>0.114800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>0.099900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11300</td>\n",
              "      <td>0.121700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11400</td>\n",
              "      <td>0.105100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.099800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11600</td>\n",
              "      <td>0.100300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11700</td>\n",
              "      <td>0.124300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11800</td>\n",
              "      <td>0.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>0.123800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12100</td>\n",
              "      <td>0.111900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12200</td>\n",
              "      <td>0.105500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12300</td>\n",
              "      <td>0.097000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12400</td>\n",
              "      <td>0.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.105600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>0.109000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12700</td>\n",
              "      <td>0.100100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12800</td>\n",
              "      <td>0.100800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12900</td>\n",
              "      <td>0.100100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.107300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13100</td>\n",
              "      <td>0.106500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13200</td>\n",
              "      <td>0.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>0.098700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13400</td>\n",
              "      <td>0.104100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.109300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13600</td>\n",
              "      <td>0.097200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13700</td>\n",
              "      <td>0.091300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13800</td>\n",
              "      <td>0.097200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13900</td>\n",
              "      <td>0.103000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.102200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14100</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14200</td>\n",
              "      <td>0.099000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14300</td>\n",
              "      <td>0.106200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14400</td>\n",
              "      <td>0.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.108700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14600</td>\n",
              "      <td>0.106600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>0.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14800</td>\n",
              "      <td>0.109000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14900</td>\n",
              "      <td>0.086400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15100</td>\n",
              "      <td>0.120200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15200</td>\n",
              "      <td>0.111800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15300</td>\n",
              "      <td>0.099900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>0.115000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.107600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15600</td>\n",
              "      <td>0.101100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage-1 checkpoint saved to: /content/baitbuster_two_stage/stage1_all_best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9 — Stage-1 training (pretrain on broad pool)"
      ],
      "metadata": {
        "id": "pnfX22rWg3nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "\n",
        "stage1_model = BertWithNumeric(MODEL_NAME, num_labels=2, num_numeric=len(NUM_FEATS))\n",
        "\n",
        "# Some transformers versions renamed eval args; try both\n",
        "try:\n",
        "    stage1_args = TrainingArguments(\n",
        "        output_dir=os.path.join(SAVE_DIR, \"stage1_all\"),\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=2,      # 2–3 is enough for pretraining\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "except TypeError:\n",
        "    stage1_args = TrainingArguments(\n",
        "        output_dir=os.path.join(SAVE_DIR, \"stage1_all\"),\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "stage1_trainer = WeightedTrainer(\n",
        "    model=stage1_model,\n",
        "    args=stage1_args,\n",
        "    train_dataset=ds_stage1,   # has sample_weight + eng_feats\n",
        ")\n",
        "stage1_trainer.train()\n",
        "\n",
        "stage1_ckpt = os.path.join(SAVE_DIR, \"stage1_all_best\")\n",
        "stage1_trainer.save_model(stage1_ckpt)\n",
        "print(\"Stage-1 checkpoint saved to:\", stage1_ckpt)"
      ],
      "metadata": {
        "id": "iCJRkzwAg6RL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "0eb40514-4d3e-4094-8771-c3b032f98628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='876' max='876' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [876/876 07:23, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Micro</th>\n",
              "      <th>Kappa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>0.030928</td>\n",
              "      <td>0.990667</td>\n",
              "      <td>0.990501</td>\n",
              "      <td>0.990667</td>\n",
              "      <td>0.981002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.009400</td>\n",
              "      <td>0.030610</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.993215</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.986430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.032135</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.993213</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.986426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.034955</td>\n",
              "      <td>0.994000</td>\n",
              "      <td>0.993895</td>\n",
              "      <td>0.994000</td>\n",
              "      <td>0.987789</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage-2 best checkpoint saved to: /content/baitbuster_two_stage/stage2_human_best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10 — Stage-2 fine-tuning (human-train) with early stopping on human-val"
      ],
      "metadata": {
        "id": "DFyW1jfag9R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# Recreate the model class and load encoder head weights from Stage-1\n",
        "stage2_model = BertWithNumeric(MODEL_NAME, num_labels=2, num_numeric=len(NUM_FEATS))\n",
        "\n",
        "# Load state dict if available\n",
        "pt_path = os.path.join(stage1_ckpt, \"pytorch_model.bin\")\n",
        "if os.path.exists(pt_path):\n",
        "    state = torch.load(pt_path, map_location=\"cpu\")\n",
        "    # Strict=False allows shape-safe load even if heads differ\n",
        "    stage2_model.load_state_dict(state, strict=False)\n",
        "else:\n",
        "    print(\"⚠️ Could not find stage-1 weights; training Stage-2 from fresh init.\")\n",
        "\n",
        "try:\n",
        "    stage2_args = TrainingArguments(\n",
        "        output_dir=os.path.join(SAVE_DIR, \"stage2_human\"),\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=4,            # 3–5 typically good\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "except TypeError:\n",
        "    stage2_args = TrainingArguments(\n",
        "        output_dir=os.path.join(SAVE_DIR, \"stage2_human\"),\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=4,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        logging_steps=100,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        seed=42,\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "stage2_trainer = Trainer(\n",
        "    model=stage2_model,\n",
        "    args=stage2_args,\n",
        "    train_dataset=ds_stage2,   # human-train only\n",
        "    eval_dataset=ds_val,       # human-val\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "stage2_trainer.train()\n",
        "\n",
        "stage2_ckpt = os.path.join(SAVE_DIR, \"stage2_human_best\")\n",
        "stage2_trainer.save_model(stage2_ckpt)\n",
        "print(\"Stage-2 best checkpoint saved to:\", stage2_ckpt)"
      ],
      "metadata": {
        "id": "zIUCef2xg_9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "58796f1d-dfef-47f7-ab3b-5b42a2274b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== HUMAN-VAL METRICS ===\n",
            "{'accuracy': 0.994,\n",
            " 'f1_macro': 0.9938946929505411,\n",
            " 'f1_micro': 0.994,\n",
            " 'kappa': 0.9877894356005789}\n",
            "\n",
            "=== HUMAN-TEST METRICS ===\n",
            "{'accuracy': 0.9933333333333333,\n",
            " 'f1_macro': 0.9932199222189477,\n",
            " 'f1_micro': 0.9933333333333333,\n",
            " 'kappa': 0.9864398689548936}\n",
            "\n",
            "Confusion matrix [rows=true, cols=pred] (0,1):\n",
            " [[648   6]\n",
            " [  4 842]]\n",
            "\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9939    0.9908    0.9923       654\n",
            "           1     0.9929    0.9953    0.9941       846\n",
            "\n",
            "    accuracy                         0.9933      1500\n",
            "   macro avg     0.9934    0.9930    0.9932      1500\n",
            "weighted avg     0.9933    0.9933    0.9933      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cell 11** - Final evaluation on human-test + save metrics"
      ],
      "metadata": {
        "id": "pOkpQp65eadc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import json, pandas as pd, os\n",
        "\n",
        "raw_val  = stage2_trainer.predict(ds_val)\n",
        "raw_test = stage2_trainer.predict(ds_test)\n",
        "\n",
        "print(\"=== HUMAN-VAL METRICS ===\")\n",
        "val_metrics = compute_metrics((raw_val.predictions, raw_val.label_ids))\n",
        "pprint(val_metrics)\n",
        "\n",
        "print(\"\\n=== HUMAN-TEST METRICS ===\")\n",
        "test_metrics = compute_metrics((raw_test.predictions, raw_test.label_ids))\n",
        "pprint(test_metrics)\n",
        "\n",
        "y_pred = raw_test.predictions.argmax(axis=1)\n",
        "print(\"\\nConfusion matrix [rows=true, cols=pred] (0,1):\\n\", confusion_matrix(raw_test.label_ids, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(raw_test.label_ids, y_pred, digits=4))\n",
        "\n",
        "pd.DataFrame([val_metrics]).to_csv(os.path.join(SAVE_DIR, \"val_metrics.csv\"), index=False)\n",
        "pd.DataFrame([test_metrics]).to_csv(os.path.join(SAVE_DIR, \"final_human_test_metrics.csv\"), index=False)\n",
        "with open(os.path.join(SAVE_DIR, \"final_human_test_metrics.json\"), \"w\") as f:\n",
        "    json.dump(test_metrics, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Wp75NZSHelvr",
        "outputId": "d08e6e38-9ac1-493b-e14b-9f5824cee45f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stage2_trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-528826130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mraw_val\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mstage2_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mraw_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage2_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stage2_trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📊 Visualization Cell — Confusion Matrix & Metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Confusion matrix\n",
        "y_true = raw_test.label_ids\n",
        "y_pred = raw_test.predictions.argmax(axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Clickbait\",\"Clickbait\"], yticklabels=[\"Not Clickbait\",\"Clickbait\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix on Human-Test\")\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for metrics\n",
        "metrics_to_plot = {\n",
        "    \"Accuracy\": test_metrics[\"accuracy\"],\n",
        "    \"F1 Macro\": test_metrics[\"f1_macro\"],\n",
        "    \"F1 Micro\": test_metrics[\"f1_micro\"],\n",
        "    \"Kappa\": test_metrics[\"kappa\"]\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=list(metrics_to_plot.keys()), y=list(metrics_to_plot.values()), palette=\"viridis\")\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Evaluation Metrics on Human-Test\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05T8ao-J6Cf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}